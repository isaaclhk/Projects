{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237021b1",
   "metadata": {},
   "source": [
    "# LLM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f147ca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\isaac\\anaconda3\\envs\\a8\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from threading import Thread\n",
    "from typing import Optional, List, Dict\n",
    "from yaml import safe_load\n",
    "\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(os.path.join(cwd, 'src'))\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import pipeline, BitsAndBytesConfig, TextIteratorStreamer\n",
    "from yaml import safe_load\n",
    "\n",
    "# Local application imports\n",
    "import RAG\n",
    "\n",
    "# Azure/OpenAI imports\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# LangChain and RAGAS imports\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.metrics import context_precision, context_recall, answer_relevancy, faithfulness\n",
    "from ragas import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7f49398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load configs\n",
    "try:\n",
    "    with open(\"config.yaml\", \"r\") as file:\n",
    "        config = safe_load(file)\n",
    "        model_variant = config.get(\"model_variant\")\n",
    "        use_quantization = config.get(\"use_quantization\")\n",
    "        embed_model = config.get(\"embed_model\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error loading config.yaml: {e}. Using default settings.\")\n",
    "    model_variant = \"4b-it\"\n",
    "    use_quantization = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5aecf5",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6ff7b1",
   "metadata": {},
   "source": [
    "### MedGemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ea1b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedGemma:\n",
    "    '''\n",
    "    A class to interact with the MedGamma model for image-text-to-text tasks.\n",
    "    This class initializes the model, loads it, \n",
    "    and provides a method to consult the model with a message and optional history.\n",
    "    \n",
    "    Attributes:\n",
    "        model_id (str): The identifier for the MedGamma model.\n",
    "        use_quantization (bool): Whether to use quantization for the model.\n",
    "        pipe (pipeline): The Hugging Face pipeline for image-text-to-text tasks.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 model_variant: str=None, \n",
    "                 use_quantization: bool=False\n",
    "                 ):\n",
    "        '''\n",
    "        Initialize the MedGamma model with the specified variant and quantization option.\n",
    "        Args:\n",
    "            model_variant (str): The variant of the MedGamma model to use.\n",
    "            use_quantization (bool): Whether to use quantization for the model.\n",
    "        '''\n",
    "        # Load environment variables and login to Hugging Face\n",
    "        load_dotenv()\n",
    "        try:\n",
    "            login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging in to Hugging Face: {e}\")\n",
    "\n",
    "        self.model_id = f\"google/medgemma-{model_variant}\"\n",
    "        self.use_quantization = use_quantization\n",
    "        self.pipe = self.load_pipeline()\n",
    "\n",
    "    def load_pipeline(self):\n",
    "        '''\n",
    "        Load the MedGamma model pipeline with optional quantization.\n",
    "        \n",
    "        Returns:\n",
    "            pipeline: A Hugging Face pipeline for image-text-to-text tasks.\n",
    "        '''\n",
    "        model_kwargs = dict(\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"cuda\"\n",
    "        )\n",
    "\n",
    "        if self.use_quantization:\n",
    "            model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"fp4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            \n",
    "        pipe = pipeline(\n",
    "        \"image-text-to-text\",\n",
    "        model=self.model_id,\n",
    "        model_kwargs=model_kwargs\n",
    "            )\n",
    "        return pipe\n",
    "\n",
    "    def consult(\n",
    "        self,\n",
    "        message: str, \n",
    "        history: List[Dict[str, str]], \n",
    "        system_prompt: str = None,\n",
    "        max_new_tokens: int = None,\n",
    "        top_k: int = None,\n",
    "        temperature: float = None,\n",
    "        image: Optional[Image.Image] = None, \n",
    "        file: Optional[List] = None\n",
    "        ):\n",
    "        '''\n",
    "        Consult the MedGamma model with a message and optional history, system prompt, image, and file attachments.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The user's message.\n",
    "            history (List[Dict[str, str]]): Conversation history.\n",
    "            system_prompt (str, optional): System prompt for the model.\n",
    "            max_new_tokens (int, optional): Maximum number of new tokens to generate.\n",
    "            top_k (int, optional): Top-k sampling parameter.\n",
    "            temperature (float, optional): Temperature for sampling.\n",
    "            image (Optional[Image.Image], optional): Image to include in the query.\n",
    "            file (Optional[List], optional): List of files to process.\n",
    "            \n",
    "        Returns:\n",
    "            Generator: Yields the model's response as it streams in.\n",
    "            '''\n",
    "        try:\n",
    "            formatted_history = [\n",
    "                {\n",
    "                    \"role\": turn[\"role\"],\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": turn[\"content\"]}]\n",
    "                }\n",
    "                for turn in history\n",
    "            ]\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": [{\n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": system_prompt\n",
    "                }]},\n",
    "                *formatted_history\n",
    "            ]\n",
    "\n",
    "            # implement RAG for files (pdf, txt, docx, csv, pptx)\n",
    "            documents = RAG.extract_text_from_pdf(file)\n",
    "                    \n",
    "            chunks = RAG.split_documents(documents)\n",
    "            vectorstore = RAG.embed_chunks(chunks, embed_model=embed_model)\n",
    "            query = message\n",
    "            results = RAG.search_similar_chunks(query, vectorstore)\n",
    "\n",
    "            retrieved_texts = [result.page_content for result in results]\n",
    "            combined_context = \"\\n\\n\".join(retrieved_texts)\n",
    "            message = f\"Use the following context to answer the question:\\n\\n{combined_context}\\n\\n\" + message\n",
    "\n",
    "            user_message = {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]}\n",
    "            messages.append(user_message)\n",
    "\n",
    "            # Set up streaming\n",
    "            streamer = TextIteratorStreamer(self.pipe.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "            input_ids = self.pipe.tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_tensors=\"pt\"\n",
    "                ).to(self.pipe.model.device)\n",
    "            \n",
    "            self.pipe.model.generation_config.do_sample = True\n",
    "            generation_kwargs = {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"streamer\": streamer,\n",
    "                \"do_sample\": True,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_k\": top_k\n",
    "                }\n",
    "\n",
    "            Thread(target=self.pipe.model.generate, kwargs=generation_kwargs).start()\n",
    "\n",
    "            output = \"\"\n",
    "            # Yield tokens as they stream in\n",
    "            for token in streamer:\n",
    "                output += token\n",
    "            \n",
    "            return output, retrieved_texts\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"_Error: {str(e)}_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97af696f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  6.00s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "medgemma = MedGemma(model_variant = model_variant,\n",
    "                    use_quantization=use_quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f4430",
   "metadata": {},
   "source": [
    "### LLM Judge  - gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd969e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file and get API key, API base and deployment name:\n",
    "load_dotenv(\".env\", override=True)\n",
    "# Set Azure OpenAI API key, base URL, and deployment name:\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "embedding_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "api_base = os.getenv(\"AZURE_OPENAI_API_BASE\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=api_base,\n",
    "    api_key=api_key,\n",
    "    api_version=api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f7bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_llm = AzureChatOpenAI(\n",
    "    openai_api_version=api_version,\n",
    "    azure_endpoint=api_base,\n",
    "    azure_deployment=deployment_name,\n",
    "    model=deployment_name,\n",
    "    validate_base_url=False,\n",
    ")\n",
    "\n",
    "# init the embeddings for answer_relevancy, answer_correctness and answer_similarity\n",
    "azure_embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_version=api_version,\n",
    "    azure_endpoint=api_base,\n",
    "    azure_deployment=embedding_name,\n",
    "    model=embedding_name,\n",
    ")\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(azure_llm)\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(azure_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a3251",
   "metadata": {},
   "source": [
    "## Load evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b75e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/eval.csv')\n",
    "questions = test_data[\"Question\"].to_list()\n",
    "ground_truth = test_data[\"Answer\"].to_list()\n",
    "\n",
    "data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "file = 'data/report.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cea7e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather generate answers\n",
    "for query in questions:\n",
    "        output, retrieved_texts = medgemma.consult(\n",
    "            message=query,\n",
    "            history=[],\n",
    "            system_prompt=\"You are a helpful medical expert.\",\n",
    "            max_new_tokens=1000,\n",
    "            top_k=50,\n",
    "            temperature=0.7,\n",
    "            image=None,\n",
    "            file=file\n",
    "            )\n",
    "        data['question'].append(query)\n",
    "        data['answer'].append(output)\n",
    "        data['contexts'].append(retrieved_texts)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67d503ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the primary medical application discussed in the paper?',\n",
       " 'answer': 'The primary medical application discussed in the paper is **radiomics-based decision support tool assists radiologists in small lung nodule classification**.\\n',\n",
       " 'contexts': ['[7] Sanne C Smid and Yves Rosseel. Sem with small samples: Two-step modeling and factor score\\nregression versus bayesian estimation with informative priors. In Small sample size solutions , pages\\n239–254. Routledge, 2020.\\n[8] Janita E Van Timmeren, Davide Cester, Stephanie Tanadini-Lang, Hatem Alkadhi, and Bettina\\nBaessler. Radiomics in medical imaging—“how-to” guide and critical reflection. Insights into imaging ,\\n11(1):91, 2020.\\n7',\n",
       "  'Kochurov, Ravin Kumar, Junpeng Lao, Christian C Luhmann, Osvaldo A Martin, et al. Pymc:\\na modern, and comprehensive probabilistic programming framework in python. PeerJ Computer\\nScience , 9:e1516, 2023.\\n[2] Benjamin Hunter, Christos Argyros, Marianna Inglese, Kristofer Linton-Reid, Ilaria Pulzato, An-\\ndrew G Nicholson, Samuel V Kemp, Pallav L. Shah, Philip L Molyneaux, Cillian McNamara, et al.\\nRadiomics-based decision support tool assists radiologists in small lung nodule classification and',\n",
       "  'lationships that can provide a quantitative approach to medical image analysis [8].\\nIn lung cancer research, radiomics-based methods are revolutionizing disease management by enhanc-\\ning early detection, diagnosis, prognosis, and treatment decision-making. A critical clinical metric for\\nevaluating cancer risk is the presence of lung nodules, with the size of these nodules being a significant\\nfactor [4]. The diameter of a nodule is measured to categorize it as either small or large.\\n1'],\n",
       " 'ground_truth': 'Classification of small lung nodules using radiomic features.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create eval dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# see sample\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6c00198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The primary medical application discussed in the paper is **radiomics-based decision support tool assists radiologists in small lung nodule classification**.\n",
      "\n",
      "1: Radiomic features are quantitative features extracted from medical images using data characterization algorithms. These features contain textural information such as spatial distribution of signal intensities and pixel interrelationships.\n",
      "\n",
      "2: The imaging modality used to collect the dataset was **CT (Computed Tomography)**.\n",
      "\n",
      "3: Based on the provided results, 51.2% of the nodules in the dataset were classified as malignant.\n",
      "\n",
      "4: Radiomics is important in lung cancer diagnosis because it offers a non-invasive complement to CT scans for classifying lung nodules. This can enhance diagnostic and clinical decision-making.\n",
      "\n",
      "5: The provided text doesn't mention the number of images used in the study. It only discusses the challenges of limited data availability in medical image analysis and the limitations of traditional statistical methods when dealing with small sample sizes.\n",
      "\n",
      "6: The text states that \"All 1998 radiomic features underwent Z-score normalization, defined as X - ¯X σ.\" This implies that 1998 features were extracted per image.\n",
      "\n",
      "7: The normalization technique applied to the features was Z-score normalization, defined as X - μ / σ.\n",
      "\n",
      "8: The provided text doesn't contain information about the number of images used for training.\n",
      "\n",
      "9: The method used for initial feature selection was **univariate frequentist logistic regression**.\n",
      "\n",
      "10: Based on the provided context, SVSS likely stands for **Spatial Vector Support System**.\n",
      "\n",
      "11: The prior distribution used for the binary indicator in SSVS is Bernoulli with a probability of 0.0015.\n",
      "\n",
      "12: Based on the provided context, the optimal value of the prior precision parameter (τ) for the Bayesian logistic regression model was **1**.\n",
      "\n",
      "13: The metric used to select the best Bayesian model was Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\n",
      "\n",
      "14: The text describes Bayesian methods, which are used for Bayesian inference.\n",
      "\n",
      "15: The AUROC score of the Bayesian model on the test set was 0.795.\n",
      "\n",
      "16: Based on the provided context, the feature that was consistently selected across models was **Annulus GLCM Entrop LLL**.\n",
      "\n",
      "17: The Lasso model selected 8 features.\n",
      "\n",
      "18: Based on the provided context, the two features selected by both SVSS and Lasso are:\n",
      "\n",
      "*   Annulus GLRLM SRLGLE LLL\n",
      "*   Annulus GLCM Entropy LLL\n",
      "\n",
      "19: The accuracy of the Lasso model on the test set was 0.728.\n",
      "\n",
      "20: The advantage of Bayesian methods over frequentist methods (like Lasso logistic regression) is that they generate posterior distributions for each parameter. This provides a range of plausible values for the parameters, considering both prior information and the likelihood of the observed data. This capability offers valuable insights, which are not available with frequentist methods.\n",
      "\n",
      "21: Traditional statistical methods often face challenges with small sample sizes, resulting in unstable and unreliable estimates.\n",
      "\n",
      "22: Based on the context provided, HDI (Highest Density Interval) represents a range of plausible values for a parameter, with a certain level of confidence. In Bayesian analysis, it's typically used to represent the posterior distribution of a parameter, given the observed data and prior beliefs. The HDI encapsulates the interval within which we are reasonably confident that the true value of the parameter lies.\n",
      "\n",
      "23: The passage doesn't explicitly state why tau was not set below 1 in the SVSS approach. However, we can infer some possible reasons based on the context:\n",
      "\n",
      "*   **Bayesian Approach:** The study uses a Bayesian logistic regression model. The value of tau is a crucial parameter in Bayesian models, representing the prior belief about the variance of the model's parameters. Setting tau below 1 would imply a stronger prior belief that the parameters are less variable, which might not be appropriate for the specific problem of small lung nodule classification. A higher tau could give the model more freedom to adjust to the data, potentially leading to better performance on the test set.\n",
      "\n",
      "*   **Model Performance:** The study's goal was to demonstrate the viability of SVSS with Bayesian logistic regression for radiomic-based small lung nodule classification. The performance on the test set was similar to the baseline Lasso logistic regression model, suggesting that the model was adequately performing. While a lower tau might theoretically improve performance in some scenarios, the fact that it performed similarly to the baseline indicates that the baseline model was already performing well enough, and a lower tau might not offer significant improvement.\n",
      "\n",
      "*   **Prior Knowledge/Expert Opinion:** The choice of tau might also be based on prior knowledge or expert opinion about the expected variability of the model's parameters. Perhaps the researchers believed that a value of 1 was a reasonable starting point for the prior distribution.\n",
      "\n",
      "In summary, while the passage doesn't provide a definitive answer, it's likely that tau was not set below 1 because the Bayesian logistic regression model, in conjunction with SVSS, performed adequately well on the test set, and a lower tau was not deemed necessary to improve performance. The choice of tau could also be based on prior knowledge or expert opinion regarding the expected variability of the model's parameters.\n",
      "24: The paper states that a limitation of Bayesian methods is **limited data in medical image analysis**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, ans in enumerate(dataset['answer']):\n",
    "    print(f'{i}: {ans}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced262fc",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc90ad4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:36<00:00,  2.71it/s]\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a58a1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.6633, 'context_recall': 0.6000, 'answer_relevancy': 0.8015, 'faithfulness': 0.7060}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
